---
{"dg-publish":true,"dg-permalink":"Redis 高可用相关问题","permalink":"/Redis 高可用相关问题/"}
---


#数据库 #Redis 

## Redis 如何保证高可用？

Redis 的三种高可用架构如下所示：


| 架构          | 专注                                   | 缺点                                |
| ------------- | -------------------------------------- | ----------------------------------- |
| 主从架构      | 避免**单点故障**，进行读写分离         | Master 宕机无法恢复                 |
| Sentinel 哨兵 | 自动故障转移达到高可用                 | 受限于单机内存和 Master 写能力      |
| Cluster 集群  | 解决**单机内存瓶颈**，提高内存的利用率，支持动态扩容 | 不支持 Pipeline、缺失事务、原子功能 |

Sentinel 解决了传统主从架构 Master 宕机后，需要手动切换的难题。具有**集群监控**、自动故障转移、配置中心、消息通知等功能。

但是 Sentinel 会引起**资源浪费**，内存可用性低：因为每台机器上的数据是一样的。

## Redis 主从同步是怎么实现的?

Redis Replication 机制支持**从**服务器 -> **主**服务器同步数据，通过该机制可以搭建 Redis 的高可用主从架构：

1. ==全量复制==：Slave 在**第一次同步**时使用的复制模式
	- Master 通过 **BGSAVE** 生成 RDB 文件，通过网络传输发送给 Slave
	- Slave 接收到后使用 RDB 文件恢复数据
2. ==增量复制==（从 Redis 2.8 开始）
	- 关键指标和组件
		- ==复制偏移量==：主服务器自己的复制偏移量、每个 Slave 的复制偏移量
		- ==复制积压缓冲区==：用于部分重同步时增量数据的**快速传输**
		- 主节点唯一ID：保证断线重连的**主身份确认**
	- **断线重连**后采取增量的恢复方式（之前用的是全量复制）

## Redis 是怎么优化断线重连同步的？

1. Redis **2.8** 以前：*SYNC* 为全量同步，对于断线重连的情况效果不好
	- RDB 文件大，网络传输时间长
	- 需要全量恢复 RDB，耗时长
2. 2.8+ 后：使用 *PSYNC* 提供两种新的复制方式：
	- 全量重同步：和 *SYNC* 类似
	- 部分重同步：只同步**断线期间**丢失的数据
		- 原理是通过**缓冲区**和**缓冲偏移量**来实现的：

![Pasted image 20220326180154.png|500](/img/user/attachments/images/Pasted%20image%2020220326180154.png)

## Sentinel 的故障转移过程？

1. ==监控==
	- 每个 Sentinel 节点定期对主从节点进行**健康检测**
	- 发送 **PING** 命令或执行 INFO 来确认节点是否存活
2. ==主观下线==
	- 如果 Sentinel 节点在**一定时间**未收到主节点响应，会将主节点标记为主观下线
3. ==客观下线==
	- 当超过配置的**法定人数**（quorum）的 Sentinel 都认为主节点不可达时
	- 会被标记为客观下线，此时将触发故障转移流程
4. ==选举 Leader Sentinel==
	- 多个 Sentinel 节点通过内部选举机制选出一个 **Leader Sentinel**（Raft或类似算法）
	- 该 Sentinel 负责执行故障转移，确保过程**有序且不会冲突**
5. ==选择新的主节点==
	- Leader Sentinel 根据从节点的**可用性、复制偏移量、延迟**等因素，选举出一个从节点作为新的主节点
6. ==执行故障转移==
	- 向选中的从节点发送 `SLAVEOF no one` 命令，脱离主从关系，成为新的主节点
	- 重新配置其他从节点，让它们**指向新的主节点**并开始同步数据
7. ==通知客户端及系统==
	- 通过发布订阅机制或其他配置通知客户端和应用程序**新的主节点地址**
	- 继续监控新的主节点

## Redis 的集群模式？

Redis 集群（*Cluster*）是在 3.0+ 提供一种去中心化的基于分布式架构设计的高可用、高性能的部署方案。

![redis-Redis - Cluster.png|400](/img/user/attachments/images/redis-Redis%20-%20Cluster.png)

1. ==数据分片==
	- 将整个键空间划分为16384个哈希槽
	- 集群中的**每个主节点**负责管理**一部分**槽位上的数据，实现水平扩展
2. ==节点结构及主从复制==
	- 集群由主节点和配备的从节点组成
	- 主：负责**读写请求**处理；从：复制主节点数据，实现**容灾和故障转移**
3. ==去中心化架构与节点通信==
	- 无单点管理节点，每个节点都保存集群的元数据信息
4. ==客户端请求路由机制==
	- 客户端可以连接集群中的**任意一个**节点，节点根据 key 对应 slot 信息
	- 如果负责该 slot 直接响应请求，否则会**重定向**让客户端访问正确的节点

## Redis Cluster 解决了什么问题？

1. ==数据容量和性能瓶颈==
	- 通过数据分片（哈希槽机制）将数据分布到多个节点，实现**水平扩展和负载均衡**
	- 解决单机内存分片的瓶颈
2. ==高可用性与自动故障转移==
	- 采用**多主多从**架构
	- 支持主从复制和自动故障转移
3. ==去中心化与无单点故障==
	- 传统集群方案依赖代理或中心节点作为调度，容易出现**单点故障**
	- 去中心化架构，增加整体的稳定性和扩展性
4. ==简化客户端访问和路由==
	- 只需连接集群中**任一**节点，无需额外代理
5. ==动态扩容和数据迁移==
	- 支持运行时添加或移除节点
	- 集群能够**动态重新分配槽位**并在线迁移数据，无需停机

## Redis Cluster 分布式哈希的实现？

Redis Cluster 的[[src/分布式/分布式哈希方案\|分布式哈希方案]]使用的是带有**虚拟节点**的一致性哈希分区
- ==数据分片==：将整个键空间划分为固定数量的 **16384** 个哈希槽
- ==节点映射==：集群中每个主节点负责管理**一定范围**的哈希槽，实现数据的均匀分布和负载均衡
- ==动态槽迁移与扩容==
	- 当新增或移除节点时，会将部分槽从一个节点迁移到另一个节点，保证数据平衡和集群扩展
	- 避免[[src/分布式/分布式哈希方案#分布式哈希表\|传统简单哈希]]和[[src/分布式/分布式哈希方案#一致性哈希\|一致性哈希]]带来的数据迁移成本和计算复杂度

## Cluster 模式下如何保证多key在同一个节点实现事务

Redis Cluster中多键事务（MULTI/EXEC）要求这些key 必须位于**同一个节点上的同一哈希槽**（hash slot）内，否则事务无法被原子执行。

通过**哈希标签**（Hash Tag）将多个不同的键强制映射到**同一个哈希槽**。

实现：
- 如果键中包含一对 `{}`，Redis只计算这对大括号之间的子串的CRC16哈希值，而忽略其他部分
- 如 `user:{1000}:name` 和 `order:{1000}:id` 最终会落在同一个哈希槽上