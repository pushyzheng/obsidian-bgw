---
{"dg-publish":true,"dg-permalink":"agency - all 概述","permalink":"/agency - all 概述/"}
---


#项目 

## 背景

1. 产品通过数据报表发现，代理商预定的失败率比较高
2. 通过系统监控发现，查询代理商价格接口的 QPS 比较高：总的量级大概在两三万左右，峰值会在四万左右

==为什么会出现预定失败的情况？==

1. 预定发生在：用户在酒店**详情页**，点击某个房型时跳转到填单页前
2. 直接原因
	- 为了保证用户**下单成功**，避免出现用户填完单、支付成功后，确认失败
	- 所以会在进入到填单页之前进行一次**实时的**价格校验（底价是否一致、库存还有、是否关房）
		- 由供应商提供，要去**实时校验**，不使用缓存
		- 如果返回错误，将会预定失败，提示「房型售完，重新选择」
3. 根本原因
	- 平台缓存价格数据与供应商系统侧，出现**数据不一致**

[[src/业务知识/酒店顺畅度#两种数据架构存在的问题\|两种数据架构存在的问题]]

## 目标与方向

- 目标
	1. 提高价格数据的准确性，降低进订失败率，从而提高订单产量
	2. 减轻调用代理商的接口压力，降低代理商的**维护成本**，这样更愿意和去哪儿合作
- 方向
	1.  不影响稳定性的前提下，尽可能地提高与代理商侧的**数据一致性**
	2.  优化缓存结构和抓取策略，适当**降低查询价格的量级**

## 方案

### 1. 系统层面上的调整

- 通过前期的调研发现
	- 原先的缓存是由比较靠近上游位置的报价中心来管理的
	- 从**系统边界层面**上看，缓存并不是他们的核心业务，它的核心业务报价的计算和产品的选货
	- 不是最靠近代理商的一侧，价格变化的通知可能会**有所延迟**
- 解决：在接入层与外部代理商之间引入一个**中间层**系统，由它来接管上游的所有流量，专注于缓存价格的管理，这样做有两个好处：
	1. 减少变价事件通知的延迟：最接近代理商侧，可以在价格发生变化后**及时更新**
	2. 将所有价格查询的流量收口到我们这，也是便于后序缓存**策略的调整**

### 2. 缓存数据结构和更新机制上的优化

==Ⅰ. 导致 QPS 放大的主要原因是：缓存记录维度不够细致==

- 原先
	- 以代理商 ID  + 酒店 ID 作为维度：没有增加入店日期和离店日期范围
	- 如果缓存中记录了两天的数据，其中一天发生变化
	- 也会导致另一天的价格数据也失效，引起 QPS 的放大
- 如何解决：
	- 减小报价缓存维度粒度：增加入店日期和离店日期

==Ⅱ. 为了提高缓存的及时性和查询命中率，将被动的刷新机制改为主动方式==

- 原先
	- 被动的刷新机制：价格变动后，并不会立即更新，而是延迟到用户进入详情页时才实时请求
	- 设计的初衷：能避免冷门数据堆积，但缓存命中率低，而且实时请求时可能由于代理商接口超时或异常会返回空的报价数据
- 解决
	- 为了尽量保证缓存的及时性，**化被动为主动**：监听到代理商变价事件发生，主动刷新价格
	- （==用户预定的特点：95 % 以上的用户都在预订 10 天内的酒店产品==）
	- 为了避免冷门数据堆积：引入**价格日历的机制**，只刷新热门日期，防止堆积
	- 极大提高上游查询报价时，缓存的命中率

### 3. 提高拉取变价的效率

原先：
- 通过去哪儿自研的 QSchedule 定时调度一台机器，来完成 2000 多个左右代理商的价格变动数据的拉取
- 缺点：速率会受限于单机资源（并发线程数、HTTP 最大连接数等）

优化：
- 思路：让集群的所有结点**都参与**到任务的调度执行当中，实现一套 Map Reduce 的方案
- 基于 QSchedule 现有的任务拆分实现：
	1. 定义两个父子任务
	2. 首先由 QSchedule Server 调度到父任务执行，父任务将在线的代理商数据暂存到 Redis 当中，并构建**分片序号列表**，作为参数调用 QSchedule Server 的 Open API 来触发子任务的调度
	3. 集群内的所有结点，在接收到调度消息后，从 Redis 中取出分配到的分片序号**对应的分片数据**，完成这一部分代理商信息的抓取
- 效果
	- 分布式并行的方案，提高拉取的效率
	- 可以在 QSchedule 后台观察到所有子任务的**运行状态和执行时间**

### 4. （亮点）承接高 QPS，引入 NIO 请求框架和响应式编程

增加了中间层系统之后，为了承接之前上游的较高的流量，做了技术上的选型和创新：

1. 考虑到该系统的特点：
	- QPS 较高：平均在两三万左右
	- I/O 密集型任务较多：大量的 HTTP 和 Redis 请求，部分接口响应时间慢
	- 对响应时间敏感
2. 使用 **NIO 网络请求框架**，实现了[[src/项目经历/qunar/agency 全链路异步化\|全链路异步化]]
	- [[src/java/Java IO#BIO、NIO、AIO 有什么区别？\|NIO]] 可以充分利用 CPU 的资源，增大吞吐量（[[src/计算机基础/IO#什么是 I O 多路复用？\|I/O 多路复用]]）
	- [[src/java/Spring#Spring DeferredResult 原理\|Spring DeferredResult]]：可以将异步操作的结果，以同步形式返回给客户端，增大服务的吞吐能力
	- AsyncHTTPClient：HTTP 请求
	- async-redis：Redis 请求
3. 其次，为了提高代码的结构性还有**可读性**，以及做一些[[src/项目经历/qunar/agency 全链路异步化#并行操作\|复杂的并行操作]]
	- 引入[[src/java/Spring Reactor#响应式编程\|响应式编程]]框架 [[src/java/Spring Reactor#Reactor\|Spring Reactor]] 进行异步编排

## 问题

- [[src/项目经历/qunar/agency-all 问题#该项目的难点？\|该项目的难点？]]
- [[src/项目经历/qunar/agency-all 问题#稳定性会从哪方面去做建设？（如何保证系统的高可用？）\|稳定性会从哪方面去做建设？（如何保证系统的高可用？）]]
- [[src/项目经历/qunar/agency-all 问题#项目的展望\|项目的展望]]
